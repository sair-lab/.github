[<img src="https://github.com/sair-lab/.github/assets/8695500/2308d56a-70f5-480f-936d-379c1a744957" height="150"/>](https://sairlab.org)

## 🙋‍♀️ Welcome to SAIR Lab 🙌

- At the intersection of **perception**, **spatial reasoning**, and **decision-making**, our research goal is to endow mobile robots with **human-level autonomy**. This vision drives us to develop **algorithms** and **systems** enabling robots to efficiently and robustly:

  - Perceive and interpret various sensory inputs such as images, point clouds, and proprioceptive data.
  - Integrate neural and symbolic representations of spatial common sense and semantic knowledge.
  - Reason and plan in real time to navigate, interact, and adapt within unstructured and dynamic environments.

## About open-source.

- SAIR Lab is leading [PyPose](https://github.com/pypose/pypose), an open-source Python library for differentiable robotics.
- This GitHub organization provides source code for **publications** and **teaching materials**.

## Find us elsewhere 🌎

[![Linkedin Badge](https://img.shields.io/badge/-LinkedIn-blue?style=flat&logo=Linkedin&logoColor=white&link=https://www.linkedin.com/company/sairlab)](https://www.linkedin.com/company/sairlab)
[![Twitter Badge](https://img.shields.io/badge/-Twitter-1ca0f1?style=flat&labelColor=1ca0f1&logo=twitter&logoColor=white&link=https://twitter.com/sairlab_org/)](https://twitter.com/sairlab_org/)
[![YouTube Badge](https://img.shields.io/badge/-YouTube-1ca0f1?style=flat&labelColor=1ca0f1&logo=youtube&logoColor=white&link=https://www.youtube.com/@sairlab/videos)](https://www.youtube.com/@sairlab/videos)
